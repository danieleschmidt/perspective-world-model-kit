# Comprehensive CI/CD workflow for PWMK
# Place this file at: .github/workflows/ci.yml

name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
  pull_request:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'
  workflow_dispatch:
    inputs:
      run_integration_tests:
        description: 'Run integration tests'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_DEFAULT_VERSION: '3.11'
  POETRY_VERSION: '1.6.1'
  PYTEST_ADDOPTS: '--color=yes'

jobs:
  # Code quality and linting
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pre-commit black isort flake8 mypy bandit safety

    - name: Cache pre-commit
      uses: actions/cache@v3
      with:
        path: ~/.cache/pre-commit
        key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}

    - name: Run pre-commit
      run: pre-commit run --all-files --show-diff-on-failure

    - name: Check code formatting (Black)
      run: black --check --diff pwmk/ tests/ scripts/

    - name: Check import sorting (isort)
      run: isort --check-only --diff pwmk/ tests/ scripts/

    - name: Lint code (flake8)
      run: flake8 pwmk/ tests/ scripts/

    - name: Type checking (mypy)
      run: mypy pwmk/ --config-file pyproject.toml

    - name: Security analysis (bandit)
      run: bandit -r pwmk/ -f json -o bandit-report.json
      continue-on-error: true

    - name: Upload bandit report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: bandit-report
        path: bandit-report.json

  # Unit tests matrix
  test:
    name: Tests (Python ${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    needs: code-quality
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11']
        include:
          - os: ubuntu-latest
            python-version: '3.12'  # Latest Python on Linux only
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y swi-prolog build-essential

    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-latest'
      run: |
        brew install swi-prolog

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test,prolog]"

    - name: Run unit tests
      run: |
        pytest tests/unit/ -v \
          --cov=pwmk \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junitxml=pytest-results.xml \
          -m "not slow and not gpu"

    - name: Run slow tests
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        pytest tests/unit/ -v \
          --cov=pwmk \
          --cov-append \
          --cov-report=xml \
          -m "slow and not gpu"

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == env.PYTHON_DEFAULT_VERSION
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          pytest-results.xml
          htmlcov/
          coverage.xml

  # Integration tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: test
    if: github.event_name == 'push' || github.event.inputs.run_integration_tests == 'true'
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: pwmk_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y swi-prolog build-essential

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test,prolog,unity]"

    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/pwmk_test
        REDIS_URL: redis://localhost:6379/0
        PWMK_TEST_MODE: true
      run: |
        pytest tests/integration/ -v \
          --cov=pwmk \
          --cov-report=xml \
          --junitxml=integration-results.xml \
          -m "integration"

    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          integration-results.xml
          coverage.xml

  # GPU tests (if runners available)
  gpu-tests:
    name: GPU Tests
    runs-on: [self-hosted, gpu]
    timeout-minutes: 30
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    continue-on-error: true  # Don't fail CI if GPU runners are unavailable
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"

    - name: Check CUDA availability
      run: python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

    - name: Run GPU tests
      run: |
        pytest tests/ -v \
          --junitxml=gpu-results.xml \
          -m "gpu"

    - name: Upload GPU test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: gpu-test-results
        path: gpu-results.xml

  # Performance benchmarks
  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"

    - name: Run performance benchmarks
      run: |
        pytest tests/performance/ -v \
          --benchmark-only \
          --benchmark-json=benchmark-results.json

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark-results.json

    - name: Performance regression check
      run: |
        python scripts/check_performance_regression.py benchmark-results.json

  # Security scanning
  security:
    name: Security Scanning
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: code-quality
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety pip-audit

    - name: Check dependencies for vulnerabilities (safety)
      run: safety check --json --output safety-report.json
      continue-on-error: true

    - name: Audit dependencies (pip-audit)
      run: pip-audit --format=json --output=pip-audit-report.json
      continue-on-error: true

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          safety-report.json
          pip-audit-report.json

    - name: Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        languages: python
        queries: security-and-quality

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2

  # Build and package
  build:
    name: Build Package
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test, security]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for version detection

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine

    - name: Build package
      run: python -m build

    - name: Check package
      run: twine check dist/*

    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dist
        path: dist/

  # Docker build
  docker:
    name: Docker Build
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [test, security]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ghcr.io/${{ github.repository }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        target: production

  # Deployment (staging)
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [build, docker]
    if: github.event_name == 'push' && github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add staging deployment commands here

  # Notification
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [test, integration-tests, benchmarks, security, build, docker]
    if: always() && github.event_name == 'push'
    
    steps:
    - name: Notify Slack on success
      if: ${{ needs.test.result == 'success' && needs.build.result == 'success' }}
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#pwmk-ci'
        message: ':white_check_mark: CI/CD pipeline succeeded for commit `${{ github.sha }}`'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

    - name: Notify Slack on failure
      if: ${{ needs.test.result == 'failure' || needs.build.result == 'failure' }}
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#pwmk-ci'
        message: ':x: CI/CD pipeline failed for commit `${{ github.sha }}`'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}